---
title: "Project"
author: "Ashutosh Gupta"
date: "Saturday, April 17, 2015"
output: html_document
---
#             Practical Machine Learning Project Summary

## Problem Defnition

### Data

The training data for this project are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har. If you use the document you create for this class for any purpose please cite them as they have been very generous in allowing their data to be used for this kind of assignment.

### What you should submit

  >*The goal of your project is to predict the manner in which they did the exercise. This is the **classe** variable in the training set. You may use any of the other variables to predict with. You should create a report describing how you built your model, how you used cross validation, what you think the expected out of sample error is, and why you made the choices you did. You will also use your prediction model to predict 20 different test cases.*

## Prepare and Analyze Data

First, we load the data provided from the link. Code for loading the data is given below:
```{r}
library(RCurl)
library(lattice)
library(ggplot2)
library(caret)
library(kernlab)

url_training <- getURL("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv")
url_testing <- getURL("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv")

df_training <- read.csv(text = url_training)
df_testing <- read.csv(text = url_testing)
dim (df_training)
```


As we can see that there are total 160 variables with 19622 records in **df_training** data frame. First six column in df_training data frame can be removed as they are not going to impact prediction e.g. first column is serial number which will have no impact on prediction.

```{r}
df_training <- df_training[,-c(1:6)]
```
 After removing first six column in data fram, we see that there are several columns which has "NA", NULL or #DIV/0! in them. We will remove all columns wherever 90% of entry in columns are filled with "NA", NULL or #DIV/0! as they will distort the results or will have no impact on prediction of **classe**. **classe** categorical variable has five labels as shown by code below:
 
 
```{r}
cutoff <- dim(df_training)[1] * 0.9

train <- !apply(df_training, 2, function(x) sum(is.na(x)) > cutoff  || sum(x=="") > cutoff)
df_training <- df_training[, train]

table(df_training$classe)

```
## Prepare training and testing data

Now, we have 53 variables and one response variable **classe**. With this reduced set, we can prepare training and testing data. We take 60% data in training and to test the preformace of model, we will take 40% of remaining data.

This can be acheived with the code give below:

```{r}
inTrain <- createDataPartition(df_training$classe,p = 0.01,list = FALSE)

training <- df_training[inTrain,]
 
testing <- df_training[-inTrain,]
```
##Algorithm

In order to reproduce same results, we set the seed. As we have 53 predictors, we will use random forest or C50 algorithm for solving this problem .C50 can deal with response variable having multiple categories. We will use random forest to solve this proble. First we will use random forest through **train** function in *caret* package. 


```{r}
set.seed(12343)
modFit <- train(classe ~ .,data = training,method = "rf", trControl=trainControl(method="cv",number=5),
                prox=TRUE)
modFit$finalModel
modFit$results
plot(modFit)
importance <- varImp(modFit,scale = FALSE)
plot(importance,top = 25)
dotPlot(importance,top = 25)
```

We can see the top 25 variables which were used in random forest tree construction.


## Check classification performance

Now, we test the model performance on testing data and measure the overall accuracy.

```{r}
modFitPred = predict ( modFit , newdata = testing)

y <- table(modFitPred,testing$classe)

accuracy <- sum(diag(y))/sum(y)
accuracy
```
As we can see that we are getting overall accuracy of 99.7% which is really good for a real life problem. We can rely on this overall accuracy as classes ("A", "B","c","D" and "E") in **classe** are balanced. Also, kappa statistic is also high which suggest that random forest algorithm has performed well on this data.As we can see form the confusion matrix that we have missed 1, 3, 6,8 and 5 times ch category A, B, C, D and E and we correctly predict 3347, 2269, 2048, 1922 and 2160 times  for category A,B,C,D and E. 
## Present Results

We were asked to generate results using the model on df_testing data frame. 

```{r}
ANSWER = predict ( modFit , newdata = df_testing)
print(ANSWER)

```

